package core

import (
	"context"
	"fmt"
	"log"
	"sync"
	"time"
	"github.com/shirou/gopsutil/v3/process"
)

// UnifiedMonitor ç»Ÿä¸€ç›‘æ§å™¨
type UnifiedMonitor struct {
	app            *App
	config         MonitoringConfig
	processes      map[int32]*MonitoredProcess
	resourceCache  map[int32]*ResourceUsage
	healthCache    map[int32]*HealthStatus
	performanceDB  map[int32][]PerformanceRecord
	eventHandlers  []EventHandler
	metrics        MetricsCollector
	mutex          sync.RWMutex
	ctx            context.Context
	cancel         context.CancelFunc
}


// MonitoredProcess è¢«ç›‘æ§çš„è¿›ç¨‹
type MonitoredProcess struct {
	PID             int32               `json:"pid"`
	Name            string              `json:"name"`
	Cmdline         string              `json:"cmdline"`
	StartTime       time.Time           `json:"start_time"`
	LastSeen        time.Time           `json:"last_seen"`
	Status          string              `json:"status"`
	Tags            map[string]string   `json:"tags"`
	Config          ProcessMonitorConfig `json:"config"`
	Health          *HealthStatus       `json:"health"`
	ResourceUsage   *ResourceUsage      `json:"resource_usage"`
	Performance     []PerformanceRecord `json:"performance"`
	RestartCount    int                 `json:"restart_count"`
	LastRestart     time.Time           `json:"last_restart"`
}

// ProcessMonitorConfig è¿›ç¨‹ç›‘æ§é…ç½®
type ProcessMonitorConfig struct {
	EnableMonitoring    bool                   `yaml:"enable_monitoring"`
	EnableHealthCheck   bool                   `yaml:"enable_health_check"`
	EnableResourceLimit bool                   `yaml:"enable_resource_limit"`
	CustomRules         []HealthCheckRule      `yaml:"custom_rules"`
	ResourceLimits      map[string]float64     `yaml:"resource_limits"`
	Tags                map[string]string      `yaml:"tags"`
	Priority            string                 `yaml:"priority"` // "high", "medium", "low"
}

// NewUnifiedMonitor åˆ›å»ºç»Ÿä¸€ç›‘æ§å™¨
func NewUnifiedMonitor(config MonitoringConfig, app *App) *UnifiedMonitor {
	ctx, cancel := context.WithCancel(context.Background())
	
	return &UnifiedMonitor{
		app:            app,
		config:         config,
		processes:      make(map[int32]*MonitoredProcess),
		resourceCache:  make(map[int32]*ResourceUsage),
		healthCache:    make(map[int32]*HealthStatus),
		performanceDB:  make(map[int32][]PerformanceRecord),
		eventHandlers:  make([]EventHandler, 0),
		ctx:            ctx,
		cancel:         cancel,
	}
}

// Start å¯åŠ¨ç»Ÿä¸€ç›‘æ§å™¨
func (um *UnifiedMonitor) Start() error {
	if !um.config.Enabled {
		return nil
	}
	
	log.Println("ğŸš€ å¯åŠ¨ç»Ÿä¸€ç›‘æ§å™¨...")
	
	// å¯åŠ¨ç›‘æ§å¾ªç¯
	go um.monitoringLoop()
	
	// å¯åŠ¨å¥åº·æ£€æŸ¥å¾ªç¯
	go um.healthCheckLoop()
	
	// å¯åŠ¨æ¸…ç†å¾ªç¯
	go um.cleanupLoop()
	
	return nil
}

// Stop åœæ­¢ç»Ÿä¸€ç›‘æ§å™¨
func (um *UnifiedMonitor) Stop() {
	um.cancel()
	
	// æ¸…ç†èµ„æº
	um.mutex.Lock()
	defer um.mutex.Unlock()
	
	um.processes = make(map[int32]*MonitoredProcess)
	um.resourceCache = make(map[int32]*ResourceUsage)
	um.healthCache = make(map[int32]*HealthStatus)
	
	log.Println("ğŸ›‘ ç»Ÿä¸€ç›‘æ§å™¨å·²åœæ­¢")
}

// monitoringLoop ç›‘æ§å¾ªç¯
func (um *UnifiedMonitor) monitoringLoop() {
	ticker := time.NewTicker(um.config.Interval)
	defer ticker.Stop()
	
	for {
		select {
		case <-um.ctx.Done():
			return
		case <-ticker.C:
			um.updateAllProcesses()
		}
	}
}

// healthCheckLoop å¥åº·æ£€æŸ¥å¾ªç¯
func (um *UnifiedMonitor) healthCheckLoop() {
	ticker := time.NewTicker(um.config.HealthCheckInterval)
	defer ticker.Stop()
	
	for {
		select {
		case <-um.ctx.Done():
			return
		case <-ticker.C:
			um.performHealthChecks()
		}
	}
}

// cleanupLoop æ¸…ç†å¾ªç¯
func (um *UnifiedMonitor) cleanupLoop() {
	ticker := time.NewTicker(time.Hour) // æ¯å°æ—¶æ¸…ç†ä¸€æ¬¡
	defer ticker.Stop()
	
	for {
		select {
		case <-um.ctx.Done():
			return
		case <-ticker.C:
			um.cleanupOldData()
		}
	}
}

// updateAllProcesses æ›´æ–°æ‰€æœ‰è¿›ç¨‹çŠ¶æ€
func (um *UnifiedMonitor) updateAllProcesses() {
	um.mutex.Lock()
	defer um.mutex.Unlock()
	
	// è·å–å½“å‰ç³»ç»Ÿè¿›ç¨‹
	allProcesses, err := process.Processes()
	if err != nil {
		log.Printf("âŒ è·å–ç³»ç»Ÿè¿›ç¨‹å¤±è´¥: %v", err)
		return
	}
	
	currentPIDs := make(map[int32]bool)
	for _, p := range allProcesses {
		pid := p.Pid
		currentPIDs[pid] = true
		
		// æ›´æ–°å·²ç›‘æ§çš„è¿›ç¨‹
		if monitored, exists := um.processes[pid]; exists {
			um.updateMonitoredProcess(monitored, p)
		}
	}
	
	// æ£€æŸ¥ä¸¢å¤±çš„è¿›ç¨‹
	for pid, monitored := range um.processes {
		if !currentPIDs[pid] {
			um.handleProcessLost(monitored)
		}
	}
	
	// æ£€æŸ¥æ˜¯å¦è¶…è¿‡æœ€å¤§ç›‘æ§è¿›ç¨‹æ•°
	if len(um.processes) > um.config.MaxMonitoredProcesses {
		um.cleanupLeastImportantProcesses()
	}
}

// updateMonitoredProcess æ›´æ–°å•ä¸ªè¢«ç›‘æ§è¿›ç¨‹
func (um *UnifiedMonitor) updateMonitoredProcess(monitored *MonitoredProcess, p *process.Process) {
	// æ›´æ–°åŸºæœ¬ä¿¡æ¯
	monitored.LastSeen = time.Now()
	
	// æ›´æ–°èµ„æºä½¿ç”¨æƒ…å†µ
	resourceUsage := um.collectResourceUsage(p)
	monitored.ResourceUsage = resourceUsage
	um.resourceCache[monitored.PID] = resourceUsage
	
	// è®°å½•æ€§èƒ½æ•°æ®
	performance := PerformanceRecord{
		Timestamp:       time.Now(),
		PID:             monitored.PID,
		CPUUsed:         resourceUsage.CPUUsed,
		MemoryUsedMB:    resourceUsage.MemoryUsedMB,
		IOReadBytes:     uint64(resourceUsage.DiskReadMB) * 1024 * 1024, // è½¬æ¢ä¸ºå­—èŠ‚
		IOWriteBytes:    uint64(resourceUsage.DiskWriteMB) * 1024 * 1024,
		PerformanceScore: resourceUsage.PerformanceScore,
		Status:          monitored.Status,
		Tags:            monitored.Tags,
	}
	
	monitored.Performance = append(monitored.Performance, performance)
	um.performanceDB[monitored.PID] = append(um.performanceDB[monitored.PID], performance)
	
	// é™åˆ¶å†å²è®°å½•å¤§å°
	if len(monitored.Performance) > um.config.PerformanceHistorySize {
		monitored.Performance = monitored.Performance[len(monitored.Performance)-um.config.PerformanceHistorySize:]
	}
	if len(um.performanceDB[monitored.PID]) > um.config.PerformanceHistorySize {
		um.performanceDB[monitored.PID] = um.performanceDB[monitored.PID][len(um.performanceDB[monitored.PID])-um.config.PerformanceHistorySize:]
	}
	
	// æ£€æŸ¥èµ„æºé™åˆ¶
	if monitored.Config.EnableResourceLimit {
		um.checkResourceLimits(monitored)
	}
}

// collectResourceUsage æ”¶é›†èµ„æºä½¿ç”¨æƒ…å†µ
func (um *UnifiedMonitor) collectResourceUsage(p *process.Process) *ResourceUsage {
	usage := &ResourceUsage{
		LastAnomaly: time.Time{},
	}
	
	// CPUä½¿ç”¨ç‡
	if cpuPercent, err := p.CPUPercent(); err == nil {
		usage.CPUUsed = cpuPercent
	}
	
	// å†…å­˜ä½¿ç”¨
	if memInfo, err := p.MemoryInfo(); err == nil {
		usage.MemoryUsedMB = int64(memInfo.RSS / 1024 / 1024)
	}
	
	// I/Oç»Ÿè®¡
	if um.config.EnableDetailedIO {
		if ioCounters, err := p.IOCounters(); err == nil {
			usage.DiskReadMB = int64(ioCounters.ReadBytes / 1024 / 1024)
			usage.DiskWriteMB = int64(ioCounters.WriteBytes / 1024 / 1024)
		}
	}
	
	// è®¡ç®—æ€§èƒ½åˆ†æ•°
	usage.calculatePerformanceScore()
	
	// æ£€æŸ¥å¼‚å¸¸
	um.detectResourceAnomalies(usage)
	
	return usage
}

// calculatePerformanceScore è®¡ç®—æ€§èƒ½åˆ†æ•°ï¼ˆç§»åŠ¨åˆ°ResourceUsageçš„æ–¹æ³•ï¼‰
func (ru *ResourceUsage) calculatePerformanceScore() {
	score := 100.0
	
	// CPUæ•ˆç‡è¯„åˆ†
	if ru.CPUExpected > 0 {
		ratio := ru.CPUUsed / ru.CPUExpected
		switch {
		case ratio > 2.0:
			score -= 30
		case ratio > 1.5:
			score -= 15
		case ratio > 1.0:
			score -= 5
		}
	}
	
	// å†…å­˜æ•ˆç‡è¯„åˆ†
	if ru.MemoryExpected > 0 {
		ratio := float64(ru.MemoryUsedMB) / float64(ru.MemoryExpected)
		switch {
		case ratio > 3.0:
			score -= 30
		case ratio > 2.0:
			score -= 15
		case ratio > 1.5:
			score -= 5
		}
	}
	
	// å¼‚å¸¸æƒ©ç½š
	if len(ru.AnomalyType) > 0 {
		score -= float64(len(ru.AnomalyType)) * 10
	}
	
	// ç¡®ä¿åˆ†æ•°åœ¨åˆç†èŒƒå›´å†…
	if score < 0 {
		score = 0
	} else if score > 100 {
		score = 100
	}
	
	ru.PerformanceScore = score
}

// detectResourceAnomalies æ£€æµ‹èµ„æºå¼‚å¸¸
func (um *UnifiedMonitor) detectResourceAnomalies(usage *ResourceUsage) {
	usage.AnomalyType = []string{}
	
	// CPUå¼‚å¸¸æ£€æµ‹
	if usage.CPUExpected > 0 && usage.CPUUsed > usage.CPUExpected*1.5 {
		usage.AnomalyType = append(usage.AnomalyType, "high_cpu")
		usage.LastAnomaly = time.Now()
	}
	
	// å†…å­˜å¼‚å¸¸æ£€æµ‹
	if usage.MemoryExpected > 0 && usage.MemoryUsedMB > usage.MemoryExpected*2 {
		usage.AnomalyType = append(usage.AnomalyType, "high_memory")
		usage.LastAnomaly = time.Now()
	}
	
	// I/Oå¼‚å¸¸æ£€æµ‹
	if um.config.EnableDetailedIO && usage.DiskWriteMB > 1000 { // è¶…è¿‡1GBå†™å…¥
		usage.AnomalyType = append(usage.AnomalyType, "high_io")
		usage.LastAnomaly = time.Now()
	}
}

// performHealthChecks æ‰§è¡Œå¥åº·æ£€æŸ¥
func (um *UnifiedMonitor) performHealthChecks() {
	um.mutex.Lock()
	defer um.mutex.Unlock()
	
	for pid, monitored := range um.processes {
		if monitored.Config.EnableHealthCheck {
			health := um.checkProcessHealth(monitored)
			monitored.Health = health
			um.healthCache[pid] = health
			
			// å¤„ç†ä¸å¥åº·çš„è¿›ç¨‹
			if !health.IsHealthy {
				um.handleUnhealthyProcess(monitored, health)
			}
		}
	}
}

// checkProcessHealth æ£€æŸ¥è¿›ç¨‹å¥åº·çŠ¶æ€
func (um *UnifiedMonitor) checkProcessHealth(monitored *MonitoredProcess) *HealthStatus {
	health := &HealthStatus{
		LastCheck: time.Now(),
		NextCheck: time.Now().Add(um.config.HealthCheckInterval),
		Issues:    []string{},
	}
	
	// åŸºç¡€è¿›ç¨‹å­˜åœ¨æ€§æ£€æŸ¥
	if p, err := process.NewProcess(monitored.PID); err != nil {
		health.IsHealthy = false
		health.Status = "not_found"
		health.Issues = append(health.Issues, "è¿›ç¨‹ä¸å­˜åœ¨")
		health.Score = 0
		return health
	} else {
		// æ£€æŸ¥è¿›ç¨‹çŠ¶æ€
		if status, err := p.Status(); err == nil {
			if len(status) > 0 && status[0] != "running" {
				health.IsHealthy = false
				health.Status = status[0]
				health.Issues = append(health.Issues, fmt.Sprintf("è¿›ç¨‹çŠ¶æ€å¼‚å¸¸: %s", status[0]))
			}
		}
		
		p, _ = process.NewProcess(monitored.PID)
	}
	
	// åº”ç”¨å¥åº·æ£€æŸ¥è§„åˆ™
	score := 100.0
	rules := um.getApplicableRules(monitored)
	for _, rule := range rules {
		if !rule.Enabled {
			continue
		}
		
		if um.evaluateHealthRule(monitored, rule) {
			health.Issues = append(health.Issues, rule.Description)
			switch rule.Severity {
			case "critical":
				score -= 25
			case "error":
				score -= 15
			case "warning":
				score -= 10
			case "info":
				score -= 5
			}
		}
	}
	
	// ç¡®å®šæœ€ç»ˆå¥åº·çŠ¶æ€
	health.Score = score
	if score >= 80 {
		health.IsHealthy = true
		health.Status = "healthy"
	} else if score >= 60 {
		health.IsHealthy = false
		health.Status = "degraded"
	} else {
		health.IsHealthy = false
		health.Status = "unhealthy"
	}
	
	return health
}

// getApplicableRules è·å–é€‚ç”¨çš„å¥åº·æ£€æŸ¥è§„åˆ™
func (um *UnifiedMonitor) getApplicableRules(monitored *MonitoredProcess) []HealthCheckRule {
	rules := um.config.HealthCheckRules
	
	// æ·»åŠ è¿›ç¨‹ç‰¹å®šçš„è§„åˆ™
	if monitored.Config.CustomRules != nil {
		rules = append(rules, monitored.Config.CustomRules...)
	}
	
	return rules
}

// evaluateHealthRule è¯„ä¼°å¥åº·æ£€æŸ¥è§„åˆ™
func (um *UnifiedMonitor) evaluateHealthRule(monitored *MonitoredProcess, rule HealthCheckRule) bool {
	var value float64
	
	switch rule.Metric {
	case "cpu":
		value = monitored.ResourceUsage.CPUUsed
	case "memory":
		value = float64(monitored.ResourceUsage.MemoryUsedMB)
	case "performance_score":
		value = monitored.ResourceUsage.PerformanceScore
	default:
		return false
	}
	
	switch rule.Operator {
	case ">":
		return value > rule.Threshold
	case "<":
		return value < rule.Threshold
	case ">=":
		return value >= rule.Threshold
	case "<=":
		return value <= rule.Threshold
	case "==":
		return value == rule.Threshold
	case "!=":
		return value != rule.Threshold
	default:
		return false
	}
}

// handleUnhealthyProcess å¤„ç†ä¸å¥åº·çš„è¿›ç¨‹
func (um *UnifiedMonitor) handleUnhealthyProcess(monitored *MonitoredProcess, health *HealthStatus) {
	// å‘é€äº‹ä»¶
	event := Event{
		Type:      EventHealthCheckFailed,
		Source:    "unified_monitor",
		Level:     LevelError,
		Message:   fmt.Sprintf("è¿›ç¨‹ %s (PID: %d) å¥åº·æ£€æŸ¥å¤±è´¥: %s", monitored.Name, monitored.PID, health.Status),
		Timestamp: time.Now(),
		PID:       monitored.PID,
		Details: map[string]interface{}{
			"health_status": health,
			"score":        health.Score,
			"issues":       health.Issues,
		},
		Tags: []string{"health", "process", monitored.Name},
	}
	
	um.emitEvent(event)
	
	// è‡ªåŠ¨é‡å¯ç­–ç•¥
	if um.config.AutoRestartAttempt && monitored.RestartCount < um.config.MaxRestartAttempts {
		// è¿™é‡Œå¯ä»¥å®ç°é‡å¯é€»è¾‘
		log.Printf("âš ï¸ è¿›ç¨‹ %s (PID: %d) ä¸å¥åº·ï¼Œè€ƒè™‘é‡å¯ç­–ç•¥", monitored.Name, monitored.PID)
	}
}

// emitEvent å‘é€äº‹ä»¶
func (um *UnifiedMonitor) emitEvent(event Event) {
	for _, handler := range um.eventHandlers {
		if err := handler.HandleEvent(event); err != nil {
			log.Printf("âŒ äº‹ä»¶å¤„ç†å¤±è´¥: %v", err)
		}
	}
}

// cleanupOldData æ¸…ç†æ—§æ•°æ®
func (um *UnifiedMonitor) cleanupOldData() {
	um.mutex.Lock()
	defer um.mutex.Unlock()
	
	cutoff := time.Now().Add(-24 * time.Hour) // æ¸…ç†24å°æ—¶å‰çš„æ•°æ®
	
	// æ¸…ç†æ€§èƒ½å†å²
	for pid, records := range um.performanceDB {
		validRecords := records[:0]
		for _, record := range records {
			if record.Timestamp.After(cutoff) {
				validRecords = append(validRecords, record)
			}
		}
		um.performanceDB[pid] = validRecords
	}
	
	// æ¸…ç†èµ„æºç¼“å­˜
	for pid, usage := range um.resourceCache {
		if usage.LastAnomaly.Before(cutoff) {
			delete(um.resourceCache, pid)
		}
	}
}

// cleanupLeastImportantProcesses æ¸…ç†æœ€ä¸é‡è¦çš„è¿›ç¨‹
func (um *UnifiedMonitor) cleanupLeastImportantProcesses() {
	// å®ç°æ¸…ç†ç­–ç•¥ï¼Œä¼˜å…ˆä¿ç•™é«˜ä¼˜å…ˆçº§çš„è¿›ç¨‹
	// è¿™é‡Œå¯ä»¥æ ¹æ®ä¼˜å…ˆçº§ã€èµ„æºä½¿ç”¨ç‡ç­‰å› ç´ æ¥å†³å®šæ¸…ç†å“ªäº›è¿›ç¨‹
}

// checkResourceLimits æ£€æŸ¥èµ„æºé™åˆ¶
func (um *UnifiedMonitor) checkResourceLimits(monitored *MonitoredProcess) {
	// å®ç°èµ„æºé™åˆ¶æ£€æŸ¥é€»è¾‘
}

// handleProcessLost å¤„ç†ä¸¢å¤±çš„è¿›ç¨‹
func (um *UnifiedMonitor) handleProcessLost(monitored *MonitoredProcess) {
	// å‘é€è¿›ç¨‹ä¸¢å¤±äº‹ä»¶
	event := Event{
		Type:      EventProcessStopped,
		Source:    "unified_monitor",
		Level:     LevelInfo,
		Message:   fmt.Sprintf("è¿›ç¨‹ %s (PID: %d) å·²åœæ­¢", monitored.Name, monitored.PID),
		Timestamp: time.Now(),
		PID:       monitored.PID,
		Details: map[string]interface{}{
			"last_seen": monitored.LastSeen,
			"uptime":    time.Since(monitored.StartTime),
		},
		Tags: []string{"process", "stopped", monitored.Name},
	}
	
	um.emitEvent(event)
	
	// ä»ç›‘æ§åˆ—è¡¨ä¸­ç§»é™¤
	delete(um.processes, monitored.PID)
	delete(um.resourceCache, monitored.PID)
	delete(um.healthCache, monitored.PID)
	delete(um.performanceDB, monitored.PID)
	
	log.Printf("ğŸ“‹ è¿›ç¨‹ %s (PID: %d) å·²ä»ç›‘æ§ä¸­ç§»é™¤", monitored.Name, monitored.PID)
}

// GetStats è·å–ç›‘æ§ç»Ÿè®¡ä¿¡æ¯ (å®ç°ProcessMonitoræ¥å£)
func (um *UnifiedMonitor) GetStats() map[string]interface{} {
	um.mutex.RLock()
	defer um.mutex.RUnlock()
	
	stats := map[string]interface{}{
		"monitored_processes": len(um.processes),
		"resource_cache_size": len(um.resourceCache),
		"health_cache_size":   len(um.healthCache),
		"performance_records":  0,
		"enabled":             um.config.Enabled,
		"max_processes":       um.config.MaxMonitoredProcesses,
	}
	
	// è®¡ç®—æ€»æ€§èƒ½è®°å½•æ•°
	for _, records := range um.performanceDB {
		stats["performance_records"] = stats["performance_records"].(int) + len(records)
	}
	
	return stats
}